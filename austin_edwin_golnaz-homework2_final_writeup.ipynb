{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distinguishing Song Genre\n",
    "Golnaz Abrishami, Austin Koenig, & Edwin Ramirez\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This study begins with two datasets (`song_data.csv` and `song_info.csv`) taken from the Spotify API. While it is tough to work with two datasets in conjunction, it would be much easier to create a single dataframe from them. So, wielded with a dataframe containing several different numerical and musical properties, the object of this report is to discover some relationship between these properties of the song between genres. For instance, it would be interesting to find that rock music was, in fact, different from country music numerically, despite them being somewhat similar in style and form. \n",
    "\n",
    "While unsupervised clustering methods might be ideal for this sort of thing, musical genres and subgenres are defined by people. Thus, it is entirely possible that two songs sound extremely similar, but are different in genre by human standards. \n",
    "\n",
    "Accordingly, this study will separate songs into genres by the words found in the name of the playlist in which these songs exist. Then, it will analyse the differences and similarites of these supervised clusters in order to determine how different each genre is from the rest. Finally, the results will be displayed and discussed in some conclusive statements. \n",
    "\n",
    "This study is useful to all those who are curious to determine whether they should be confined to only one genre of music, or whether it is safe to venture into the vast infinitum of the musical world. Shall they find solice in another genre? Is there only one genre that will quench their auditory thirst? At the end of this report, they will certainly find out.\n",
    "\n",
    "### Features of Data\n",
    "\n",
    "**Song Popularity**\n",
    "\n",
    "The popularity of an artist's track relative to the rest of their discography. The value will be between 0 and 100, with 100 being the most popular.\n",
    "\n",
    "The popularity of a track is a value between 0 and 100, with 100 being the most popular. The popularity is calculated by algorithm and is based, in the most part, on the total number of plays the track has had and how recent those plays are.\n",
    "\n",
    "**Song Duration (ms)**\n",
    "\n",
    "Contains data on the length of time the song is in milliseconds.\n",
    "\n",
    "**Acousticness**\n",
    "\n",
    "A measurement from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents highest confidence the track is acoustic.\n",
    "\n",
    "**Danceability**\n",
    "\n",
    "Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of type float is returned between 0.0 and 1.0, where 0.0 is least danceable and 1.0 is most danceable.\n",
    "\n",
    "**Energy**\n",
    "\n",
    "Energy is a measure of type float that is between 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Energetic tracks can be described as fast, loud, and noisy. For example, death metal has high energy, while a slow jazz would register low on the scale.\n",
    "\n",
    "**Instrumentalness**\n",
    "\n",
    "Predicts whether a track contains no vocals by returning a value of type float that is between 0.0 and 1.0. “Ooh” and “aah” sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly “vocal”. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.\n",
    "\n",
    "**Liveness**\n",
    "\n",
    "Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.\n",
    "\n",
    "**Loudness**\n",
    "\n",
    "The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typical range between -60 and 0 db\n",
    "\n",
    "**Audio Mode**\n",
    "\n",
    "Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.\n",
    "\n",
    "**Speechiness**\n",
    "\n",
    "Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.\n",
    "\n",
    "**Tempo**\n",
    "\n",
    "The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.\n",
    "\n",
    "**Time Signature**\n",
    "\n",
    "An estimated overall time signature of a track. The time signature (meter) is a notational convention to specify how many beats are in each bar (or measure).\n",
    "\n",
    "**Key Signature**\n",
    "\n",
    "Key signature is represented on a scale, where integers are mapped to pitches using standard Pitch Class notation . E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. \n",
    "\n",
    "**Audio Valence**\n",
    "\n",
    "A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).\n",
    "\n",
    "**Artist Name**\n",
    "\n",
    "Artist that wrote the song\n",
    "\n",
    "**Album Names**\n",
    "Album that the song was released on\n",
    "\n",
    "**Playlist**\n",
    "Playlist that song originates from. **Note: Some songs may be part of multiple playlists**\n",
    "\n",
    "### Importing Necessary Libraries\n",
    "\n",
    "The libraries that are needed for this analysis will now be imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Data\n",
    "\n",
    "Now, the datasets will be loaded and combined into a single pandas `DataFrame` object with all of the desired features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "songdata_df = DataFrame(pd.read_csv(\"song_data.csv\", index_col = 0))\n",
    "songinfo_df = DataFrame(pd.read_csv(\"song_info.csv\", index_col = 0))\n",
    "songs_df = pd.concat([songdata_df, songinfo_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the Data by Genre\n",
    "\n",
    "If the dataset is split by genre, then it will be much easier to make distinctions between them since their numerical properties won't be lost in a mist of a multitude of other songs belonging to other genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'alt_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-40edfb19ea1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mcountry_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcountry_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_mode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mpop_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpop_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_mode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0malt_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malt_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_mode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0mrap_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrap_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_mode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mlatin_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlatin_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_mode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'alt_df' is not defined"
     ]
    }
   ],
   "source": [
    "rock_df = songs_df[songs_df['playlist'].str.contains(\"Rock\")]\n",
    "rock_df.insert(rock_df.shape[1], \"Genre\", \"Rock\", True) \n",
    "\n",
    "#Create Country DataFrame\n",
    "country_df = songs_df[songs_df['playlist'].str.contains(\"Country\")]\n",
    "country_df.insert(country_df.shape[1], \"Genre\", \"Country\", True)\n",
    "\n",
    "\n",
    "#Create pop dataframe\n",
    "pop_df = songs_df[songs_df['playlist'].str.contains(\"Pop\")]\n",
    "pop_df.insert(pop_df.shape[1], \"Genre\", \"Pop\", True)\n",
    "\n",
    "\n",
    "#Collect playlists from rap and hip hop\n",
    "rap_df = songs_df[(songs_df['playlist'].str.contains(\"Rap\")) | (songs_df['playlist'].str.contains(\"Hop\")) ]\n",
    "rap_df.insert(rap_df.shape[1], \"Genre\", \"Rap\", True)\n",
    "\n",
    "\n",
    "#Collect playlists from rap and hip hop\n",
    "rap_df = songs_df[(songs_df['playlist'].str.contains(\"Rap\")) | (songs_df['playlist'].str.contains(\"Hop\")) ]\n",
    "rap_df.insert(rap_df.shape[1], \"Genre\", \"Rap\", True)\n",
    "\n",
    "#Electronic\n",
    "electronic_df = songs_df[ (songs_df['playlist'].str.contains(\"House\")) | (songs_df['playlist'].str.contains(\"Instrumental\")) | (songs_df['playlist'].str.contains(\"Chill\"))]\n",
    "electronic_df.insert(electronic_df.shape[1], \"Genre\", \"Electronic\", True)\n",
    "\n",
    "#Latin music\n",
    "latin_df = songs_df[(songs_df['playlist'].str.contains(\"Latin\")) | \n",
    "                    (songs_df['playlist'].str.contains(\"Mex\")) | \n",
    "                    (songs_df['playlist'].str.contains(\"Columbia\")) | \n",
    "                    (songs_df['playlist'].str.contains(\"Puerto\")) | \n",
    "                    (songs_df['playlist'].str.contains(\"Brazil\")) | \n",
    "                    (songs_df['playlist'].str.contains(\"Salsa\")) | \n",
    "                    (songs_df['playlist'].str.contains(\"Bachata\")) | \n",
    "                    (songs_df['playlist'].str.contains(\"Regg\")) | \n",
    "                    (songs_df['playlist'].str.contains(\"Chicano\"))]\n",
    "latin_df.insert(latin_df.shape[1], \"Genre\", \"Latin\", True)\n",
    "\n",
    "\n",
    "\n",
    "# create value counts for each genre\n",
    "rock_mode = rock_df.audio_mode.value_counts()\n",
    "country_mode = country_df.audio_mode.value_counts()\n",
    "pop_mode = pop_df.audio_mode.value_counts()\n",
    "alt_mode = alt_df.audio_mode.value_counts()\n",
    "rap_mode = rap_df.audio_mode.value_counts()\n",
    "latin_mode = latin_df.audio_mode.value_counts()\n",
    "electronic_mode = electronic_df.audio_mode.value_counts()\n",
    "\n",
    "\n",
    "all_genres_df=pd.concat([latin_df, electronic_df,alt_df,rap_df,pop_df,country_df,rock_df],sort=False)\n",
    "\n",
    "#Create column that maps numerical key values to musical notation\n",
    "conditions = [(all_genres_df.key == 0),\n",
    "              (all_genres_df.key == 1),\n",
    "              (all_genres_df.key == 2),\n",
    "              (all_genres_df.key == 3),\n",
    "              (all_genres_df.key == 4),\n",
    "              (all_genres_df.key == 5),\n",
    "              (all_genres_df.key == 6),\n",
    "              (all_genres_df.key == 7),\n",
    "              (all_genres_df.key == 8),\n",
    "              (all_genres_df.key == 9),\n",
    "              (all_genres_df.key == 10),\n",
    "              (all_genres_df.key == 11)]\n",
    "\n",
    "key_sigs = [\"C\", \"C#\", \"D\", \"D#\",\"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\"]\n",
    "\n",
    "all_genres_df['key_note'] = np.select(conditions, key_sigs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, take an initial peek at the data within our new data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_genres_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration\n",
    "\n",
    "Taking a look at a pairs plot of our dataset, there was little correlation found between the features. There are many numerical variables, few of which tell much about the data at hand. Thus, this study also splits the data into genres (generalized by playlists in Spotify) and studies the actual differences between genres of music as well.\n",
    "\n",
    "The authors of this report have chosen the following variables to be the main features of study, proposing that they create the largest distinction between genres:\n",
    "\n",
    "- Audio Valence (`audio_valence`)\n",
    "- Energy (`energy`)\n",
    "- Key (`key`)\n",
    "- Modality (`audio_mode`)\n",
    "- Danceability (`danceability`)\n",
    "- Acousticness (`acousticness`)\n",
    "- Loudness (`loudness`)\n",
    "- Speechiness (`speechiness`)\n",
    "- Tempo (`tempo`)\n",
    "\n",
    "Observe some histograms of the continuous numeric features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(2, 2, figsize=(15,15), sharex=False)\n",
    "ax1 = sns.distplot( all_genres_df.audio_valence , color=\"green\", ax=axes[0, 0], rug= True, \n",
    "                   rug_kws={\"alpha\": 0.02, \"color\": \"brown\"})\n",
    "ax2 = sns.distplot( all_genres_df.energy , color=\"red\", ax=axes[0, 1], rug= True, \n",
    "                   rug_kws={\"alpha\": 0.02, \"color\": \"blue\"})\n",
    "ax3 = sns.distplot( all_genres_df.danceability , color=\"gold\", ax=axes[1, 0], rug= True, \n",
    "                   rug_kws={\"alpha\": 0.02, \"color\": \"red\"})\n",
    "ax4 = sns.distplot( all_genres_df.tempo, color=\"teal\", ax=axes[1, 1], rug= True, \n",
    "                   rug_kws={\"alpha\": 0.02, \"color\": \"gold\"})\n",
    "\n",
    "ax1.set_xlabel('Audio Valence',weight = \"bold\")\n",
    "ax2.set_xlabel('Energy', weight = \"bold\")\n",
    "ax3.set_xlabel('Danceability', weight = \"bold\")\n",
    "ax4.set_xlabel('Tempo', weight = \"bold\")\n",
    "\n",
    "for item in ([ax1.xaxis.label, ax1.yaxis.label] +\n",
    "             ax1.get_xticklabels() + ax1.get_yticklabels()):\n",
    "    item.set_fontsize(16)\n",
    "for item in ([ax2.xaxis.label, ax2.yaxis.label] +\n",
    "             ax2.get_xticklabels() + ax2.get_yticklabels()):\n",
    "    item.set_fontsize(16)\n",
    "\n",
    "for item in ([ax3.xaxis.label, ax3.yaxis.label] +\n",
    "             ax3.get_xticklabels() + ax3.get_yticklabels()):\n",
    "    item.set_fontsize(16)\n",
    "\n",
    "for item in ([ax4.xaxis.label, ax4.yaxis.label] +\n",
    "             ax4.get_xticklabels() + ax4.get_yticklabels()):\n",
    "    item.set_fontsize(16)\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Audio valence, energy, and danceability seem to have a near normal distribution. Tempo seems to be very erratic, and speechiness and acousticness seem to be very skewed. These observations may help to match a distribution to each set of data in order to gain a better understanding of the underlying natural phenomena which affect these values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Now that the general shape of the data is known, it may now be useful to go more in depth with the analysis. The following bar plot shows a very interesting phenomenon related to our study: each genre, for the most part, uses major keys in their songs more often than minor keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# count within each genre the number of major and minor keys\n",
    "minor = np.array([rock_mode[0], country_mode[0], pop_mode[0], alt_mode[0],rap_mode[0], latin_mode[0], electronic_mode[0]])\n",
    "major = np.array([rock_mode[1], country_mode[1], pop_mode[1], alt_mode[1],rap_mode[1], latin_mode[1], electronic_mode[1]])\n",
    "total_modes = major + minor\n",
    "\n",
    "# genre labels\n",
    "genres = [\"rock\", \"country\", \"pop\", \"alternative\", \"rap\", \"latin\", \"electronic\"]\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "sns.set_context({\"figure.figsize\": (15, 10)})\n",
    "\n",
    "#Plot 1 - background - \"total\" (top) series\n",
    "ax1 = sns.barplot(x = genres, y = total_modes, color = \"red\")\n",
    "\n",
    "#Plot 2 - overlay - \"bottom\" series\n",
    "bottom_plot = sns.barplot(x = genres, y = major, color = \"#0000A3\")\n",
    "\n",
    "\n",
    "topbar = plt.Rectangle((0,0),1,1,fc=\"red\", edgecolor = 'none')\n",
    "bottombar = plt.Rectangle((0,0),1,1,fc='#0000A3',  edgecolor = 'none')\n",
    "l = plt.legend([bottombar, topbar], ['Major', 'Minor'], loc=1, ncol = 2, prop={'size':16})\n",
    "l.draw_frame(False)\n",
    "\n",
    "#Optional code - Make plot look nicer\n",
    "sns.despine(left=True)\n",
    "bottom_plot.set_ylabel(\"Song Count\", weight = \"bold\")\n",
    "bottom_plot.set_xlabel(\"Genre\", weight = \"bold\")\n",
    "\n",
    "#Set fonts to consistent 16pt size\n",
    "for item in ([bottom_plot.xaxis.label, bottom_plot.yaxis.label] +\n",
    "             bottom_plot.get_xticklabels() + bottom_plot.get_yticklabels()):\n",
    "    item.set_fontsize(16)\n",
    "\n",
    "X = np.array(range(0,7))\n",
    "for i in X:\n",
    "    ax1.text(i,major[i]/2, major[i], color='white', ha=\"center\", fontsize = 16, weight = \"bold\")\n",
    "    bottom_plot.text(i, major[i] + minor[i]/3.5, minor[i], color =\"white\", ha = \"center\", fontsize = 16, weight = \"bold\")\n",
    "\n",
    "ax1.set_title(\"Modality Distribution Within Genres\", fontsize = 20, weight = \"bold\", ha= \"center\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, minor keys tend to sound meloncholy and major keys tend to sound cheerful. However, listening to any particular song (major or minor) in most genres makes it really hard to tell whether the song is in a major or minor scale by this measure. As it turns out, audio valence is a great feature to capture this idea. \n",
    "\n",
    "Due to the phenomenon shown in the bar plot, perhaps then the modality of a song may not be a good distinguisher of genres. This is even more apparent upon the realization that it is, in fact, a binary feature. A song can only be either major or minor: there's no grey area here. Thus, how could it possibly make a distinction between, say, several-hundred different genres?\n",
    "\n",
    "As different genres often refer to different moods or states of mind, then it is not absurd to claim that audio valence can be a good indication of genre. However, it certainly is not the only trait of a song that dictates the genre. Thus being so, observe the following heat maps: the x-axis determines the genre, the y-axis determines the key, and the color of the cell determines the audio valence. There is a heat map for major songs, one for minor songs, and one including both major and minor songs. Between each genre, how do both the key and audio valence vary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "genres_minor = all_genres_df[all_genres_df.audio_mode == 0]\n",
    "genres_major = all_genres_df[all_genres_df.audio_mode == 1]\n",
    "\n",
    "minor_valence = pd.pivot_table(genres_minor,values='audio_valence', \n",
    "                                           index='key_note', columns='Genre', aggfunc=np.median)\n",
    "major_valence = pd.pivot_table(genres_major,values='audio_valence', \n",
    "                                           index='key_note', columns='Genre', aggfunc=np.median)\n",
    "\n",
    "songdata_valence = pd.pivot_table(all_genres_df,values='audio_valence', index='key_note', \n",
    "                                  columns='Genre', aggfunc=np.median)\n",
    "\n",
    "# Minor songs\n",
    "\n",
    "sns.set_context({\"figure.figsize\": (15, 10)})\n",
    "ax = sns.heatmap(minor_valence, cmap= 'PuOr',annot= True, fmt='0.2', annot_kws={'fontsize':20}, \n",
    "                 cbar_kws={'label': 'Positivity'})\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.ax.tick_params(labelsize=20)\n",
    "\n",
    "\n",
    "ax.figure.axes[-1].yaxis.label.set_size(20)\n",
    "\n",
    "plt.yticks(rotation = 0)\n",
    "\n",
    "plt.title('Do Minor Key Signatures Affect Positivity in Music?',fontsize=30)\n",
    "plt.ylabel('Key\\n',fontsize=20, weight = \"bold\").set_rotation(90)\n",
    "plt.xlabel('Genres',fontsize=20, weight = \"bold\").set_rotation(0)\n",
    "\n",
    "#Set fonts to consistent 16pt size\n",
    "for item in ([ax.xaxis.label, ax.yaxis.label] +\n",
    "             ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(16)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Major songs\n",
    "\n",
    "sns.set_context({\"figure.figsize\": (15, 10)})\n",
    "ax = sns.heatmap(major_valence, cmap= 'PuOr',annot= True, fmt='0.2', annot_kws={'fontsize':20}, \n",
    "                 cbar_kws={'label': 'Positivity'})\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.ax.tick_params(labelsize=20)\n",
    "\n",
    "\n",
    "ax.figure.axes[-1].yaxis.label.set_size(20)\n",
    "\n",
    "plt.yticks(rotation = 0)\n",
    "\n",
    "plt.title('Do Major Key Signatures Affect Positivity in Music?',fontsize=30)\n",
    "plt.ylabel('Key\\n',fontsize=20, weight = \"bold\").set_rotation(90)\n",
    "plt.xlabel('Genres',fontsize=20, weight = \"bold\").set_rotation(0)\n",
    "\n",
    "#Set fonts to consistent 16pt size\n",
    "for item in ([ax.xaxis.label, ax.yaxis.label] +\n",
    "             ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(16)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Major and minor songs\n",
    "\n",
    "sns.set_context({\"figure.figsize\": (15, 10)})\n",
    "ax = sns.heatmap(songdata_valence, cmap= 'PuOr',annot= True, fmt='0.2', annot_kws={'fontsize':20}, \n",
    "                 cbar_kws={'label': 'Audio Valence(Musical Positiveness)'})\n",
    "\n",
    "plt.yticks(rotation = 0)\n",
    "plt.title('What Makes Music More Positive?',fontsize=30)\n",
    "plt.ylabel('Key\\n',fontsize=20,labelpad=18).set_rotation(0)\n",
    "plt.xlabel('Genres',fontsize=20,labelpad=5).set_rotation(0)\n",
    "#Set fonts to consistent 16pt size\n",
    "for item in ([ax.xaxis.label, ax.yaxis.label] +\n",
    "             ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(16)\n",
    "#ax.figure.axes[-1].yaxis.label.set_size(18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, by the presented data, latin music is the \"happiest\" type of music. Inversely, rap and pop music seem to be the \"saddest\" types of music. Also notice that audio valence is not greatly affected by the key of a song. This may have to do with modality (as discussed above); so, it may be that key and modality together carry some sort of important information, the likes of which is purportedly captured by audio valence also. \n",
    "\n",
    "What about a better way to look at audio valence than just numbers? A better intuition of how audio valence differs between genres can be gathered from the following violin plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "\n",
    "vln_plot = sns.violinplot(x='Genre', y='audio_valence', data=all_genres_df, inner=\"quartile\",alpha=.1)\n",
    "vln_plot = sns.stripplot(x='Genre', y='audio_valence', data=all_genres_df, jitter=0.3,alpha=.4)\n",
    "\n",
    "plt.title('The Great Differences in Audio Valence Between Genres',fontsize=30)\n",
    "plt.ylabel('Audio Valence',fontsize=15).set_rotation(90)\n",
    "plt.xlabel('Genre',fontsize=15).set_rotation(0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot gives a much clearer understanding of the distribution of each genre's data. Latin's audio valence seems to be well above the generic average. This seems apparently so with country, and inversely so with electronic music as well. Counterintuitively, pop seems to have a very low average of audio valence. To compare each genre even further, consider the following novel plot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_latin_df = latin_df.mean(axis = 0, numeric_only = True)\n",
    "avg_electronic_df = electronic_df.mean(axis = 0, numeric_only = True)\n",
    "avg_alt_df = alt_df.mean(axis = 0, numeric_only = True)\n",
    "avg_rap_df = rap_df.mean(axis = 0, numeric_only = True)\n",
    "avg_pop_df = pop_df.mean(axis = 0, numeric_only = True)\n",
    "avg_country_df = country_df.mean(axis = 0, numeric_only = True)\n",
    "avg_rock_df = rock_df.mean(axis = 0, numeric_only = True)\n",
    "avg_df = pd.concat([avg_latin_df, avg_electronic_df, avg_alt_df, avg_rap_df, avg_pop_df, avg_country_df, avg_rock_df])\n",
    "avg_latin_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
